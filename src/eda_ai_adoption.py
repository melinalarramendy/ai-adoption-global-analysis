import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
import os
warnings.filterwarnings('ignore')

# ConfiguraciÃ³n de visualizaciÃ³n
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("ğŸ” Iniciando AnÃ¡lisis Exploratorio - AI Tool Adoption")

# Cargar dataset - VERIFICAR RUTA
try:
    df = pd.read_csv('data/ai_adoption_dataset.csv')
    print("âœ… Dataset cargado correctamente")
except FileNotFoundError:
    # Intentar rutas alternativas
    try:
        df = pd.read_csv('../data/ai_adoption_dataset.csv')
        print("âœ… Dataset cargado correctamente (desde ../data/)")
    except:
        print("âŒ No se pudo cargar el dataset. Verifica la ruta.")
        exit()

# InspecciÃ³n inicial
print("ğŸ“Š DIMENSIONES DEL DATASET:")
print(f"Filas: {df.shape[0]}, Columnas: {df.shape[1]}")

print("\nğŸ“‹ PRIMERAS FILAS:")
print(df.head())

print("\nğŸ” INFORMACIÃ“N GENERAL:")
print(df.info())

print("\nğŸ“ˆ ESTADÃSTICAS DESCRIPTIVAS:")
print(df.describe(include='all'))

print("\nğŸ¯ ESTRUCTURA DE COLUMNAS:")
for i, col in enumerate(df.columns, 1):
    print(f"{i}. {col} - {df[col].dtype}")
   
    
# FunciÃ³n para anÃ¡lisis de valores nulos
def analizar_valores_nulos(df):
    nulos = df.isnull().sum()
    porcentaje_nulos = (nulos / len(df)) * 100
    
    print("ğŸ“Š ANÃLISIS DE VALORES NULOS:")
    for col in df.columns:
        print(f"{col}: {nulos[col]} nulos ({porcentaje_nulos[col]:.2f}%)")
    
    return nulos

# Analizar valores nulos
nulos = analizar_valores_nulos(df)

# Limpieza bÃ¡sica
df_clean = df.copy()

# Eliminar columnas con mÃ¡s del 50% de valores nulos
umbral_nulos = 0.5
columnas_a_eliminar = nulos[nulos > len(df) * umbral_nulos].index
df_clean = df_clean.drop(columns=columnas_a_eliminar)

print(f"ğŸ—‘ï¸ Columnas eliminadas: {list(columnas_a_eliminar)}")

# Llenar valores nulos segÃºn el tipo de dato
for col in df_clean.columns:
    if df_clean[col].isnull().sum() > 0:
        if df_clean[col].dtype in ['float64', 'int64']:
            df_clean[col].fillna(df_clean[col].median(), inplace=True)
        else:
            df_clean[col].fillna('Desconocido', inplace=True)

print("âœ… Limpieza de datos completada")

def analisis_univariado(df):
    print("ğŸ“Š ANÃLISIS UNIVARIADO")
    
    # Seleccionar columnas numÃ©ricas y categÃ³ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    categorical_cols = df.select_dtypes(include=['object']).columns
    
    print(f"ğŸ”¢ Columnas numÃ©ricas: {len(numeric_cols)}")
    print(f"ğŸ“ Columnas categÃ³ricas: {len(categorical_cols)}")
    
    return numeric_cols, categorical_cols

numeric_cols, categorical_cols = analisis_univariado(df_clean)

# CORRECCIÃ“N 1: VisualizaciÃ³n de distribuciones numÃ©ricas MEJORADA
if len(numeric_cols) > 0:
    print("\nğŸ“Š CREANDO GRÃFICOS DE DISTRIBUCIÃ“N...")
    
    # Crear figura con subplots adecuados
    n_cols = min(3, len(numeric_cols))
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))
    
    # Aplanar el array de axes para iteraciÃ³n fÃ¡cil
    if n_rows > 1 or n_cols > 1:
        axes = axes.ravel()
    else:
        axes = [axes]
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            # Tratamiento especial para 'year' - usar grÃ¡fico de barras
            if col == 'year':
                df_clean[col].value_counts().sort_index().plot(kind='bar', ax=axes[i], alpha=0.7)
                axes[i].set_title(f'DistribuciÃ³n de {col} (Variable CategÃ³rica)')
            else:
                df_clean[col].hist(bins=30, ax=axes[i], alpha=0.7, color='skyblue', edgecolor='black')
                axes[i].set_title(f'DistribuciÃ³n de {col}')
            
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('Frecuencia')
    
    # Ocultar ejes vacÃ­os
    for i in range(len(numeric_cols), len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('distribuciones_numericas.png', dpi=300, bbox_inches='tight')
    print("âœ… GrÃ¡fico de distribuciones guardado: distribuciones_numericas.png")
    plt.show()

# AnÃ¡lisis de variables categÃ³ricas
if len(categorical_cols) > 0:
    print("\nğŸ“ ANÃLISIS DE VARIABLES CATEGÃ“RICAS:")
    for col in categorical_cols:
        # Solo analizar columnas con menos de 50 valores Ãºnicos
        if df_clean[col].nunique() <= 50:
            print(f"\nğŸ“Š AnÃ¡lisis de {col}:")
            print(f"Valores Ãºnicos: {df_clean[col].nunique()}")
            print(f"Top 5 valores:")
            print(df_clean[col].value_counts().head())

# CORRECCIÃ“N 2: AnÃ¡lisis de correlaciÃ³n MEJORADO
if len(numeric_cols) > 1:
    print("\nğŸ”¥ CREANDO MAPA DE CORRELACIONES...")
    
    plt.figure(figsize=(10, 8))
    correlation_matrix = df_clean[numeric_cols].corr()
    
    # Crear mÃ¡scara para el triÃ¡ngulo superior
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    
    sns.heatmap(correlation_matrix, 
                mask=mask,
                annot=True, 
                cmap='coolwarm', 
                center=0,
                square=True,
                fmt='.3f',  # 3 decimales para ver mejor
                cbar_kws={"shrink": .8})
    plt.title('Mapa de CorrelaciÃ³n de Variables NumÃ©ricas')
    plt.tight_layout()
    plt.savefig('correlaciones.png', dpi=300, bbox_inches='tight')
    print("âœ… Mapa de correlaciones guardado: correlaciones.png")
    plt.show()
    
    # Identificar correlaciones fuertes
    strong_correlations = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            if abs(correlation_matrix.iloc[i, j]) > 0.7:
                strong_correlations.append((
                    correlation_matrix.columns[i],
                    correlation_matrix.columns[j],
                    correlation_matrix.iloc[i, j]
                ))
    
    print("\nğŸ”¥ CORRELACIONES FUERTES (>0.7):")
    if strong_correlations:
        for corr in strong_correlations:
            print(f"  {corr[0]} - {corr[1]}: {corr[2]:.3f}")
    else:
        print("  No se encontraron correlaciones fuertes")

# CORRECCIÃ“N 3: AnÃ¡lisis por industria (CON NOMBRES CORRECTOS)
if 'industry' in df_clean.columns and 'adoption_rate' in df_clean.columns:
    print("\nğŸ­ CREANDO ANÃLISIS POR INDUSTRIA...")
    
    plt.figure(figsize=(12, 8))
    industry_adoption = df_clean.groupby('industry')['adoption_rate'].mean().sort_values(ascending=False)
    
    industry_adoption.plot(kind='bar')
    plt.title('Tasa de AdopciÃ³n de IA por Industria')
    plt.xlabel('Industria')
    plt.ylabel('Tasa de AdopciÃ³n Promedio')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('adopcion_por_industria.png', dpi=300, bbox_inches='tight')
    print("âœ… GrÃ¡fico por industria guardado: adopcion_por_industria.png")
    plt.show()

# CORRECCIÃ“N 4: ExportaciÃ³n para Power BI (CON NOMBRES CORRECTOS)
print("\nğŸ“¤ EXPORTANDO DATOS PARA POWER BI...")

# Asegurar que existe la carpeta data
if not os.path.exists('data'):
    os.makedirs('data')
    print("ğŸ“ Carpeta 'data/' creada")

# Exportar dataset limpio
df_clean.to_csv('data/ai_adoption_cleaned.csv', index=False)
print("âœ… Dataset limpio exportado: data/ai_adoption_cleaned.csv")

# Crear datasets resumidos para el dashboard
try:
    # CORRECCIÃ“N: Usar nombres de columnas en minÃºsculas
    # AnÃ¡lisis por industria (si existe la columna)
    if 'industry' in df_clean.columns:
        industry_summary = df_clean.groupby('industry').agg({
            'adoption_rate': 'mean',
            'daily_active_users': 'mean'
        }).round(3)
        
        # Agregar conteo de empresas por industria
        industry_summary['company_count'] = df_clean.groupby('industry').size()
        
        industry_summary.to_csv('data/industry_summary.csv')
        print("âœ… Resumen por industria exportado: data/industry_summary.csv")
    
    # AnÃ¡lisis por paÃ­s/regiÃ³n (si existe)
    if 'country' in df_clean.columns:
        country_summary = df_clean.groupby('country').agg({
            'adoption_rate': 'mean',
            'daily_active_users': 'mean'
        }).round(3)
        country_summary['company_count'] = df_clean.groupby('country').size()
        country_summary.to_csv('data/country_summary.csv')
        print("âœ… Resumen por paÃ­s exportado: data/country_summary.csv")
    
    # Dataset para tendencias temporales (usando 'year')
    if 'year' in df_clean.columns:
        yearly_trends = df_clean.groupby('year').agg({
            'adoption_rate': 'mean',
            'daily_active_users': 'mean'
        }).round(3)
        yearly_trends['company_count'] = df_clean.groupby('year').size()
        yearly_trends.to_csv('data/yearly_trends.csv')
        print("âœ… Tendencias anuales exportadas: data/yearly_trends.csv")
    
    # Top 10 anÃ¡lisis (para grÃ¡ficos especÃ­ficos)
    if 'adoption_rate' in df_clean.columns and 'industry' in df_clean.columns:
        # Top 10 industrias con mayor adopciÃ³n
        top_industries = df_clean.groupby('industry')['adoption_rate'].mean().nlargest(10).reset_index()
        top_industries.to_csv('data/top_10_industries.csv', index=False)
        print("âœ… Top 10 industrias exportado: data/top_10_industries.csv")
    
    print("\nğŸ‰ Todos los datos exportados exitosamente!")
    print("ğŸ“Š Archivos listos en carpeta 'data/':")
    print("   - ai_adoption_cleaned.csv (dataset completo)")
    print("   - industry_summary.csv (resumen por industria)")
    print("   - country_summary.csv (resumen por paÃ­s)")
    print("   - yearly_trends.csv (tendencias anuales)")
    print("   - top_10_industries.csv (top 10 industrias)")

except Exception as e:
    print(f"âŒ Error en exportaciÃ³n: {e}")
    import traceback
    print(f"ğŸ” Detalles: {traceback.format_exc()}")

print("\nğŸš€ AnÃ¡lisis Exploratorio COMPLETADO!")